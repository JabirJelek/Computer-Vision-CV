{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YQz-SHihptXA",
        "outputId": "cc15275e-1344-4530-a94a-19ed79c61a8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.204-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.17-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.204-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.17-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.204 ultralytics-thop-2.0.17\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics  # This includes YOLOv8\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMQbHrqtpyWI",
        "outputId": "4d64ae0e-27e3-4ff1-bafa-d5d1762489e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iQsLIIIqZMX"
      },
      "outputs": [],
      "source": [
        "# Define paths\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "from IPython.display import display, Image\n",
        "import matplotlib.pyplot as plt\n",
        "# dir_path = \"/content/drive/MyDrive/ObjectDetection-Tasks\" - - - - - ORIGINAL\n",
        "# filepath = os.path.join(dir_path, \"Task-for-model-2\")\n",
        "# dataset_path = os.path.join(filepath, 'retrainDataset/combinedRetrainData')\n",
        "# dir = os.path.join(dataset_path, \"yoloSamLabelData\")\n",
        "\n",
        "\n",
        "dir_path = \"/content/drive/MyDrive/ObjectDetection-Tasks\"\n",
        "filepath = os.path.join(dir_path, \"APD-Detection\")\n",
        "dataset_path = os.path.join(filepath, 'dataset0')\n",
        "dir = os.path.join(dataset_path, 'data0/yoloSamLabel')\n",
        "\n",
        "\n",
        "labels_source = os.path.join(dataset_path, \"labelData\")\n",
        "images_source = os.path.join(dataset_path,  \"image\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNQu_dQ3tx0P"
      },
      "source": [
        "### Using custom build syntax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWfH5IgqpluP"
      },
      "outputs": [],
      "source": [
        "# def visualize_yolo_annotation(image_path, annotation_path, class_names):\n",
        "#     \"\"\"\n",
        "#     Visualizes YOLO annotations on an image.\n",
        "\n",
        "#     Args:\n",
        "#         image_path (str): Path to the image file.\n",
        "#         annotation_path (str): Path to the YOLO annotation file (.txt).\n",
        "#         class_names (list): A list of class names, indexed by class_id.\n",
        "#     \"\"\"\n",
        "#     # Load the image\n",
        "#     image = cv2.imread(image_path)\n",
        "#     if image is None:\n",
        "#         print(f\"Error: Could not load image from {image_path}\")\n",
        "#         return None\n",
        "\n",
        "#     # Convert BGR to RGB for proper color display\n",
        "#     image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "#     h, w, _ = image_rgb.shape\n",
        "\n",
        "#     # Check if annotation file exists\n",
        "#     if not os.path.exists(annotation_path):\n",
        "#         print(f\"Error: Annotation file not found at {annotation_path}\")\n",
        "#         return None\n",
        "\n",
        "#     # Read the annotation file\n",
        "#     try:\n",
        "#         with open(annotation_path, 'r') as f:  # Fixed: 'txt' -> 'r'\n",
        "#             annotations = f.readlines()\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error reading annotation file: {e}\")\n",
        "#         return None\n",
        "\n",
        "#     # Process each annotation\n",
        "#     for annotation in annotations:\n",
        "#         parts = annotation.strip().split()\n",
        "#         if len(parts) != 5:\n",
        "#             print(f\"Warning: Invalid annotation format in line: {annotation.strip()}\")\n",
        "#             continue\n",
        "\n",
        "#         try:\n",
        "#             class_id = int(parts[0])\n",
        "#             center_x_norm, center_y_norm, width_norm, height_norm = map(float, parts[1:])\n",
        "\n",
        "#             # Convert normalized coordinates to absolute pixel coordinates\n",
        "#             center_x = int(center_x_norm * w)\n",
        "#             center_y = int(center_y_norm * h)\n",
        "#             box_width = int(width_norm * w)\n",
        "#             box_height = int(height_norm * h)\n",
        "\n",
        "#             # Calculate top-left and bottom-right corner coordinates\n",
        "#             x1 = int(center_x - box_width / 2)\n",
        "#             y1 = int(center_y - box_height / 2)\n",
        "#             x2 = int(center_x + box_width / 2)\n",
        "#             y2 = int(center_y + box_height / 2)\n",
        "\n",
        "#             # Ensure coordinates are within image boundaries\n",
        "#             x1 = max(0, min(x1, w))\n",
        "#             y1 = max(0, min(y1, h))\n",
        "#             x2 = max(0, min(x2, w))\n",
        "#             y2 = max(0, min(y2, h))\n",
        "\n",
        "#             # Draw the bounding box\n",
        "#             color = (0, 255, 0)  # Green color for bounding box\n",
        "#             thickness = 2\n",
        "#             cv2.rectangle(image_rgb, (x1, y1), (x2, y2), color, thickness)\n",
        "\n",
        "#             # Display the class name\n",
        "#             if class_id < len(class_names):\n",
        "#                 label = class_names[class_id]\n",
        "#                 font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "#                 font_scale = 0.7\n",
        "#                 font_thickness = 2\n",
        "\n",
        "#                 # Get text size for background\n",
        "#                 text_size = cv2.getTextSize(label, font, font_scale, font_thickness)[0]\n",
        "\n",
        "#                 # Draw background rectangle for text\n",
        "#                 cv2.rectangle(image_rgb, (x1, y1 - text_size[1] - 10),\n",
        "#                              (x1 + text_size[0], y1), color, -1)\n",
        "\n",
        "#                 # Put text\n",
        "#                 cv2.putText(image_rgb, label, (x1, y1 - 5),\n",
        "#                            font, font_scale, (0, 0, 0), font_thickness, cv2.LINE_AA)\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error processing annotation: {e}\")\n",
        "#             continue\n",
        "\n",
        "#     return image_rgb\n",
        "\n",
        "# def display_image(image):\n",
        "#     \"\"\"Display image using matplotlib (better for Colab)\"\"\"\n",
        "#     plt.figure(figsize=(12, 8))\n",
        "#     plt.imshow(image)\n",
        "#     plt.axis('off')\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_3_o8uRsK0j"
      },
      "outputs": [],
      "source": [
        "# # Example usage for Colab:\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Mount Google Drive (if files are in Drive)\n",
        "#     # from google.colab import drive\n",
        "#     # drive.mount('/content/drive')\n",
        "\n",
        "#     # Example paths - UPDATE THESE WITH YOUR ACTUAL PATHS\n",
        "#     image_file = images_source  # Replace with your actual image path\n",
        "#     annotation_file = dir  # Replace with your actual annotation path\n",
        "\n",
        "#     # Alternative: Upload files directly to Colab\n",
        "#     # from google.colab import files\n",
        "#     # uploaded = files.upload()\n",
        "#     # image_file = list(uploaded.keys())[0]  # Use first uploaded file as image\n",
        "#     annotation_file = image_file.replace('.jpg', '.txt').replace('.png', '.txt')\n",
        "\n",
        "#     # Define your class names in the correct order\n",
        "#     my_class_names = [\"fork\", \"rice\", \"menu\", \"innerBox\",\"outerBox\"]\n",
        "\n",
        "#     # Visualize the annotations\n",
        "#     result_image = visualize_yolo_annotation(image_file, annotation_file, my_class_names)\n",
        "\n",
        "#     if result_image is not None:\n",
        "#         display_image(result_image)\n",
        "#         print(\"Annotation visualization completed successfully!\")\n",
        "#     else:\n",
        "#         print(\"Failed to visualize annotations.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27D6OGt1tuTX"
      },
      "source": [
        "### Using ultralytics utilities to display annotated image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3v5PRuCtt_A"
      },
      "outputs": [],
      "source": [
        "# Function utilizing ultralytics to display segmentation of images\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import math\n",
        "\n",
        "def visualize_yolo_segmentation(image_path, annotation_path, class_names):\n",
        "    \"\"\"\n",
        "    Visualizes YOLO segmentation annotations on an image.\n",
        "    \"\"\"\n",
        "    # Load image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Error: Could not load image from {image_path}\")\n",
        "        return None\n",
        "\n",
        "    # Convert BGR to RGB\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    h, w, _ = image_rgb.shape\n",
        "\n",
        "    # Check if annotation file exists\n",
        "    if not os.path.exists(annotation_path):\n",
        "        print(f\"Error: Annotation file not found at {annotation_path}\")\n",
        "        return None\n",
        "\n",
        "    # Define colors for different classes\n",
        "    colors = [\n",
        "        (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0),\n",
        "        (255, 0, 255), (0, 255, 255), (128, 0, 0), (0, 128, 0),\n",
        "        (128, 128, 0), (128, 0, 128), (0, 128, 128), (128, 128, 128)\n",
        "    ]\n",
        "\n",
        "    # Read and process annotations\n",
        "    try:\n",
        "        with open(annotation_path, 'r') as f:\n",
        "            annotations = f.readlines()\n",
        "\n",
        "        for i, annotation in enumerate(annotations):\n",
        "            parts = annotation.strip().split()\n",
        "\n",
        "            if not parts:\n",
        "                continue\n",
        "\n",
        "            class_id = int(parts[0])\n",
        "            polygon_coords = list(map(float, parts[1:]))\n",
        "\n",
        "            # Convert normalized coordinates to pixel coordinates\n",
        "            pixel_coords = []\n",
        "            for j in range(0, len(polygon_coords), 2):\n",
        "                if j + 1 < len(polygon_coords):\n",
        "                    x = int(polygon_coords[j] * w)\n",
        "                    y = int(polygon_coords[j + 1] * h)\n",
        "                    pixel_coords.append([x, y])\n",
        "\n",
        "            if len(pixel_coords) < 3:\n",
        "                continue\n",
        "\n",
        "            polygon = np.array(pixel_coords, dtype=np.int32)\n",
        "            color = colors[class_id % len(colors)]\n",
        "\n",
        "            # Draw filled polygon with transparency\n",
        "            mask = np.zeros_like(image_rgb)\n",
        "            cv2.fillPoly(mask, [polygon], color)\n",
        "            alpha = 0.3\n",
        "            image_rgb = cv2.addWeighted(image_rgb, 1, mask, alpha, 0)\n",
        "\n",
        "            # Draw polygon outline\n",
        "            cv2.polylines(image_rgb, [polygon], True, color, 2)\n",
        "\n",
        "            # Add class label\n",
        "            if class_id < len(class_names):\n",
        "                label = class_names[class_id]\n",
        "                M = cv2.moments(polygon)\n",
        "                if M[\"m00\"] != 0:\n",
        "                    cx = int(M[\"m10\"] / M[\"m00\"])\n",
        "                    cy = int(M[\"m01\"] / M[\"m00\"])\n",
        "\n",
        "                    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "                    font_scale = 0.7\n",
        "                    font_thickness = 2\n",
        "\n",
        "                    text_size = cv2.getTextSize(label, font, font_scale, font_thickness)[0]\n",
        "                    cv2.rectangle(image_rgb,\n",
        "                                (cx - text_size[0]//2 - 5, cy - text_size[1] - 5),\n",
        "                                (cx + text_size[0]//2 + 5, cy + 5),\n",
        "                                color, -1)\n",
        "                    cv2.putText(image_rgb, label,\n",
        "                              (cx - text_size[0]//2, cy),\n",
        "                              font, font_scale, (255, 255, 255), font_thickness, cv2.LINE_AA)\n",
        "\n",
        "        return image_rgb\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing annotations: {e}\")\n",
        "        return None\n",
        "\n",
        "def calculate_grid_size(num_images, max_cols=4):\n",
        "    \"\"\"\n",
        "    Calculate optimal grid dimensions for subplots.\n",
        "\n",
        "    Args:\n",
        "        num_images (int): Number of images to display\n",
        "        max_cols (int): Maximum number of columns in the grid\n",
        "\n",
        "    Returns:\n",
        "        tuple: (rows, cols) for subplot grid\n",
        "    \"\"\"\n",
        "    cols = min(num_images, max_cols)\n",
        "    rows = math.ceil(num_images / cols)\n",
        "    return rows, cols\n",
        "\n",
        "def visualize_segmentation_batch(images_dir, labels_dir, class_names, num_images=4, max_cols=4, figsize=(20, 15)):\n",
        "    \"\"\"\n",
        "    Visualize multiple segmentation images in an adaptive grid.\n",
        "\n",
        "    Args:\n",
        "        images_dir (str): Directory containing images\n",
        "        labels_dir (str): Directory containing annotation files\n",
        "        class_names (list): List of class names\n",
        "        num_images (int): Number of images to visualize (use None for all images)\n",
        "        max_cols (int): Maximum number of columns in the grid\n",
        "        figsize (tuple): Figure size (width, height)\n",
        "    \"\"\"\n",
        "    # Get all image files\n",
        "    image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    # Limit number of images if specified\n",
        "    if num_images is not None:\n",
        "        image_files = image_files[:num_images]\n",
        "\n",
        "    if not image_files:\n",
        "        print(\"No image files found in the directory.\")\n",
        "        return\n",
        "\n",
        "    num_images = len(image_files)\n",
        "    rows, cols = calculate_grid_size(num_images, max_cols)\n",
        "\n",
        "    # Create subplot grid\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
        "\n",
        "    # If only one image, convert axes to array for consistent indexing\n",
        "    if num_images == 1:\n",
        "        axes = np.array([axes])\n",
        "    if rows == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "    if cols == 1:\n",
        "        axes = axes.reshape(-1, 1)\n",
        "\n",
        "    # Flatten axes array for easy iteration\n",
        "    axes_flat = axes.flatten()\n",
        "\n",
        "    processed_count = 0\n",
        "    for i, img_file in enumerate(image_files):\n",
        "        if processed_count >= len(axes_flat):\n",
        "            break\n",
        "\n",
        "        image_path = os.path.join(images_dir, img_file)\n",
        "        annotation_path = os.path.join(labels_dir, os.path.splitext(img_file)[0] + '.txt')\n",
        "\n",
        "        if os.path.exists(annotation_path):\n",
        "            result_image = visualize_yolo_segmentation(image_path, annotation_path, class_names)\n",
        "            if result_image is not None:\n",
        "                axes_flat[processed_count].imshow(result_image)\n",
        "                axes_flat[processed_count].set_title(f\"{img_file}\", fontsize=10)\n",
        "                axes_flat[processed_count].axis('off')\n",
        "                processed_count += 1\n",
        "            else:\n",
        "                print(f\"Failed to process: {img_file}\")\n",
        "        else:\n",
        "            print(f\"Annotation not found for: {img_file}\")\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for j in range(processed_count, len(axes_flat)):\n",
        "        axes_flat[j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Successfully processed {processed_count} out of {num_images} images\")\n",
        "\n",
        "def visualize_single_segmentation(image_path, annotation_path, class_names, figsize=(12, 8)):\n",
        "    \"\"\"\n",
        "    Visualize a single segmentation image.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file\n",
        "        annotation_path (str): Path to the annotation file\n",
        "        class_names (list): List of class names\n",
        "        figsize (tuple): Figure size\n",
        "    \"\"\"\n",
        "    result_image = visualize_yolo_segmentation(image_path, annotation_path, class_names)\n",
        "\n",
        "    if result_image is not None:\n",
        "        plt.figure(figsize=figsize)\n",
        "        plt.imshow(result_image)\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Failed to visualize segmentation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw_QPgdmisQY"
      },
      "source": [
        "### History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_RP8S8LXw3cW"
      },
      "outputs": [],
      "source": [
        "# # Syntax to utilize the function above\n",
        "\n",
        "# # Define your directories\n",
        "# images_dir = images_source\n",
        "# labels_dir = dir\n",
        "\n",
        "# # # Define your class names\n",
        "# my_class_names = [\"fork\", \"rice\", \"menu\", \"innerBox\", \"outerBox\"]\n",
        "\n",
        "# # Get a list of image files\n",
        "# image_extensions = ('.jpg', '.jpeg', '.png')\n",
        "# image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(image_extensions)]\n",
        "\n",
        "# # Process the first image\n",
        "# if image_files:\n",
        "#     first_image_name = image_files[0]\n",
        "#     image_file = os.path.join(images_dir, first_image_name)\n",
        "#     annotation_file = os.path.join(labels_dir, os.path.splitext(first_image_name)[0] + '.txt')\n",
        "\n",
        "#     print(f\"Processing: {first_image_name}\")\n",
        "#     print(f\"Image path: {image_file}\")\n",
        "#     print(f\"Annotation path: {annotation_file}\")\n",
        "\n",
        "#     if os.path.exists(annotation_file):\n",
        "#         # Visualize segmentation\n",
        "#         result_image = visualize_yolo_segmentation(image_file, annotation_file, my_class_names)\n",
        "\n",
        "#         if result_image is not None:\n",
        "#             display_segmentation_result(result_image, f\"Segmentation: {first_image_name}\")\n",
        "\n",
        "#             # Also show annotation file content for verification\n",
        "#             print(\"\\nAnnotation file content:\")\n",
        "#             with open(annotation_file, 'r') as f:\n",
        "#                 for i, line in enumerate(f):\n",
        "#                     parts = line.strip().split()\n",
        "#                     if parts:\n",
        "#                         class_id = int(parts[0])\n",
        "#                         num_points = (len(parts) - 1) // 2\n",
        "#                         print(f\"Line {i}: Class {class_id} ({my_class_names[class_id]}), Points: {num_points}\")\n",
        "#         else:\n",
        "#             print(\"Failed to visualize segmentation.\")\n",
        "#     else:\n",
        "#         print(f\"Annotation file not found: {annotation_file}\")\n",
        "# else:\n",
        "#     print(\"No image files found in the directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SmMlpC52xB53"
      },
      "outputs": [],
      "source": [
        "# Function to display multiple images\n",
        "# def visualize_multiple_segmentations(images_dir, labels_dir, class_names, num_images=4):\n",
        "#     \"\"\"Visualize multiple segmentation images\"\"\"\n",
        "#     image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "#     fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
        "#     axes = axes.ravel()\n",
        "\n",
        "#     processed = 0\n",
        "#     for i, img_file in enumerate(image_files):\n",
        "#         if processed >= num_images:\n",
        "#             break\n",
        "\n",
        "#         image_path = os.path.join(images_dir, img_file)\n",
        "#         annotation_path = os.path.join(labels_dir, os.path.splitext(img_file)[0] + '.txt')\n",
        "\n",
        "#         if os.path.exists(annotation_path):\n",
        "#             result_image = visualize_yolo_segmentation(image_path, annotation_path, class_names)\n",
        "#             if result_image is not None:\n",
        "#                 axes[processed].imshow(result_image)\n",
        "#                 axes[processed].set_title(f\"Segmentation: {img_file}\", fontsize=12)\n",
        "#                 axes[processed].axis('off')\n",
        "#                 processed += 1\n",
        "\n",
        "#     # Hide unused subplots\n",
        "#     for j in range(processed, len(axes)):\n",
        "#         axes[j].axis('off')\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# # Usage for multiple images\n",
        "# # Visualize exactly 10 images (will show error if less than 10 available)\n",
        "# visualize_segmentation_batch(images_dir, labels_dir, my_class_names, num_images=10, max_cols=4)\n",
        "\n",
        "# # Visualize ALL images in the directory\n",
        "# visualize_segmentation_batch(images_dir, labels_dir, my_class_names, num_images=None, max_cols=3)\n",
        "\n",
        "# # Different grid configurations\n",
        "# visualize_segmentation_batch(images_dir, labels_dir, my_class_names, num_images=8, max_cols=2)  # 4x2 grid\n",
        "# visualize_segmentation_batch(images_dir, labels_dir, my_class_names, num_images=12, max_cols=6)  # 2x6 grid\n",
        "\n",
        "# Visualize just one specific image\n",
        "# image_file = os.path.join(images_dir, \"your_specific_image.jpg\")\n",
        "# annotation_file = os.path.join(labels_dir, \"your_specific_image.txt\")\n",
        "# visualize_single_segmentation(image_file, annotation_file, my_class_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-xgD_vlvmDw"
      },
      "outputs": [],
      "source": [
        "# # Check the problematic annotation file\n",
        "# annotation_file = os.path.join(labels_dir, os.path.splitext(first_image_name)[0] + '.txt')\n",
        "\n",
        "# print(f\"Contents of {annotation_file}:\")\n",
        "# with open(annotation_file, 'r') as f:\n",
        "#     lines = f.readlines()\n",
        "#     for i, line in enumerate(lines):\n",
        "#         parts = line.strip().split()\n",
        "#         print(f\"Line {i}: {parts} (Number of values: {len(parts)})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mFBd7c0uyvFn"
      },
      "outputs": [],
      "source": [
        "# # Auto visualize in these group with error labelling\n",
        "\n",
        "# # # Define your directories\n",
        "# images_dir = images_source\n",
        "# labels_dir = dir\n",
        "\n",
        "# # # Define your class names\n",
        "# my_class_names = [\"fork\", \"rice\", \"menu\", \"innerBox\", \"outerBox\"]\n",
        "\n",
        "# def smart_visualization(images_dir, labels_dir, class_names, desired_count=10):\n",
        "#     \"\"\"\n",
        "#     Smart visualization that handles cases where fewer images are available.\n",
        "#     \"\"\"\n",
        "#     image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "#     available_count = len(image_files)\n",
        "#     actual_count = min(desired_count, available_count)\n",
        "\n",
        "#     print(f\"Requested: {desired_count} images\")\n",
        "#     print(f\"Available: {available_count} images\")\n",
        "#     print(f\"Will display: {actual_count} images\")\n",
        "\n",
        "#     if actual_count == 0:\n",
        "#         print(\"No images to display.\")\n",
        "#         return\n",
        "\n",
        "#     visualize_segmentation_batch(images_dir, labels_dir, class_names, num_images=actual_count)\n",
        "\n",
        "# # Usage - automatically adapts to available images\n",
        "# smart_visualization(images_dir, labels_dir, my_class_names, desired_count=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpEy-BEKf34O"
      },
      "source": [
        "### Display batched images from auto_annotate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nc7mCvPEfT2x",
        "outputId": "25d2d903-cf84-4507-d1b7-f393c300ede8"
      },
      "outputs": [],
      "source": [
        "# This function to display image from auto_annotate function in other notebook, with batched processing, where the amount of images is more than 200\n",
        "\n",
        "# # Define your directories\n",
        "images_dir = images_source\n",
        "labels_dir = dir\n",
        "\n",
        "# # Define your class names\n",
        "my_class_names = [\"person_with_helmet_forklift\", \"person_with_mask_forklift\",\"person_with_mask_helmet_forklift\",\"person_with_mask_nonForklift\",\"person_without_mask_helmet_forklift\",\"person_without_mask_nonForklift\"]                              #[\"fork\", \"rice\", \"menu\", \"innerBox\", \"outerBox\"]\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import os\n",
        "\n",
        "def smart_visualization_batched(images_dir, labels_dir, class_names, desired_count=20, batch_size=20):\n",
        "    \"\"\"\n",
        "    Smart visualization that handles large numbers of images by processing them in batches.\n",
        "    Correctly iterates through all images across multiple batches.\n",
        "    \"\"\"\n",
        "    # Get all image files\n",
        "    image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    # Limit to desired_count and ensure we don't exceed available images\n",
        "    available_count = len(image_files)\n",
        "    actual_count = min(desired_count, available_count)\n",
        "\n",
        "    print(f\"Requested: {desired_count} images\")\n",
        "    print(f\"Available: {available_count} images\")\n",
        "    print(f\"Will display: {actual_count} images in batches of {batch_size}\")\n",
        "\n",
        "    if actual_count == 0:\n",
        "        print(\"No images to display.\")\n",
        "        return\n",
        "\n",
        "    # Process and display images in batches\n",
        "    for batch_start in range(0, actual_count, batch_size):\n",
        "        batch_end = min(batch_start + batch_size, actual_count)\n",
        "        current_batch_files = image_files[batch_start:batch_end]  # Correctly slice the next batch\n",
        "\n",
        "        print(f\"Displaying batch {batch_start//batch_size + 1}: images {batch_start} to {batch_end-1}\")\n",
        "\n",
        "        # Process the current batch\n",
        "        for img_file in current_batch_files:\n",
        "            image_path = os.path.join(images_dir, img_file)\n",
        "            annotation_path = os.path.join(labels_dir, os.path.splitext(img_file)[0] + '.txt')\n",
        "\n",
        "            if os.path.exists(annotation_path):\n",
        "                # Use your existing visualization function for a single image\n",
        "                visualize_single_segmentation(image_path, annotation_path, class_names)\n",
        "            else:\n",
        "                print(f\"Annotation not found for: {img_file}\")\n",
        "\n",
        "# Load your model (assuming you have a model for visualization)\n",
        "#model = YOLO(\"yolo11n.pt\")  # Load your preferred model\n",
        "\n",
        "# Usage - automatically adapts to available images\n",
        "smart_visualization_batched(images_dir, labels_dir, my_class_names, desired_count=602, batch_size=30)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bNQu_dQ3tx0P"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
