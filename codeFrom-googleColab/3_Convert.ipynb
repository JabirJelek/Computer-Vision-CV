{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOTtBUOJnl1EqfTuznDMIBd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Import library"],"metadata":{"id":"kuoq5h1_aCyP"}},{"cell_type":"code","source":["from ultralytics.data.split import autosplit"],"metadata":{"id":"xHud0ZuhaAIS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Function from ultralytics to autosplit"],"metadata":{"id":"LN0F2ZL9buHn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zh-qK_rxZrhc"},"outputs":[],"source":["# Function from ultralytics to autosplit\n","\n","\"\"\"\n","The function below is structured like this\n","\n","autosplit(\n","    path: Path = DATASETS_DIR / \"coco8/images\",\n","    weights: tuple[float, float, float] = (0.9, 0.1, 0.0),\n","    annotated_only: bool = False,\n",") -> None\n","\"\"\"\n","\n","def autosplit(\n","    path: Path = DATASETS_DIR / \"coco8/images\",\n","    weights: tuple[float, float, float] = (0.9, 0.1, 0.0),\n","    annotated_only: bool = False,\n",") -> None:\n","    \"\"\"\n","    Automatically split a dataset into train/val/test splits and save the resulting splits into autosplit_*.txt files.\n","\n","    Args:\n","        path (Path): Path to images directory.\n","        weights (tuple): Train, validation, and test split fractions.\n","        annotated_only (bool): If True, only images with an associated txt file are used.\n","\n","    Examples:\n","        Split images with default weights\n","        >>> from ultralytics.data.split import autosplit\n","        >>> autosplit()\n","\n","        Split with custom weights and annotated images only\n","        >>> autosplit(path=\"path/to/images\", weights=(0.8, 0.15, 0.05), annotated_only=True)\n","    \"\"\"\n","    path = Path(path)  # images dir\n","    files = sorted(x for x in path.rglob(\"*.*\") if x.suffix[1:].lower() in IMG_FORMATS)  # image files only\n","    n = len(files)  # number of files\n","    random.seed(0)  # for reproducibility\n","    indices = random.choices([0, 1, 2], weights=weights, k=n)  # assign each image to a split\n","\n","    txt = [\"autosplit_train.txt\", \"autosplit_val.txt\", \"autosplit_test.txt\"]  # 3 txt files\n","    for x in txt:\n","        if (path.parent / x).exists():\n","            (path.parent / x).unlink()  # remove existing\n","\n","    LOGGER.info(f\"Autosplitting images from {path}\" + \", using *.txt labeled images only\" * annotated_only)\n","    for i, img in TQDM(zip(indices, files), total=n):\n","        if not annotated_only or Path(img2label_paths([str(img)])[0]).exists():  # check label\n","            with open(path.parent / txt[i], \"a\", encoding=\"utf-8\") as f:\n","                f.write(f\"./{img.relative_to(path.parent).as_posix()}\" + \"\\n\")  # add image to txt file"]},{"cell_type":"markdown","source":["### Function from ultralytics to convert from segmentation label result into bounding boxes. segment2boxes"],"metadata":{"id":"80fpkN2jbr63"}},{"cell_type":"code","source":["# Function from ultralytics to convert from segmentation label result into bounding boxes\n","def segments2boxes(segments):\n","    \"\"\"\n","    Convert segment labels to box labels, i.e. (cls, xy1, xy2, ...) to (cls, xywh).\n","\n","    Args:\n","        segments (list): List of segments where each segment is a list of points, each point is [x, y] coordinates.\n","\n","    Returns:\n","        (np.ndarray): Bounding box coordinates in xywh format.\n","    \"\"\"\n","    boxes = []\n","    for s in segments:\n","        x, y = s.T  # segment xy\n","        boxes.append([x.min(), y.min(), x.max(), y.max()])  # cls, xyxy\n","    return xyxy2xywh(np.array(boxes))  # cls, xywh"],"metadata":{"id":"3qowtF1kaTtP","executionInfo":{"status":"ok","timestamp":1759282570460,"user_tz":-420,"elapsed":13,"user":{"displayName":"Raihan Farid","userId":"10819737156521457225"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["### Function from ultralytics to auto_annotate using 'custom model', with the help of sam model to make segmentation data. Now, i want to know if i need to convert this  from mask2segment / directly segment2boxes."],"metadata":{"id":"h-99GpwvbosX"}},{"cell_type":"code","source":["# Function from ultralytics to auto_annotate using 'custom model', with the help of sam model to make segmentation data. Now, i want to know if i need to convert this  from mask2segment / directly segment2boxes.\n","\n","def auto_annotate(\n","    data: str | Path,\n","    det_model: str = \"yolo11x.pt\",\n","    sam_model: str = \"sam_b.pt\",\n","    device: str = \"\",\n","    conf: float = 0.25,\n","    iou: float = 0.45,\n","    imgsz: int = 640,\n","    max_det: int = 300,\n","    classes: list[int] | None = None,\n","    output_dir: str | Path | None = None,\n",") -> None:\n","    \"\"\"\n","    Automatically annotate images using a YOLO object detection model and a SAM segmentation model.\n","\n","    This function processes images in a specified directory, detects objects using a YOLO model, and then generates\n","    segmentation masks using a SAM model. The resulting annotations are saved as text files in YOLO format.\n","\n","    Args:\n","        data (str | Path): Path to a folder containing images to be annotated.\n","        det_model (str): Path or name of the pre-trained YOLO detection model.\n","        sam_model (str): Path or name of the pre-trained SAM segmentation model.\n","        device (str): Device to run the models on (e.g., 'cpu', 'cuda', '0'). Empty string for auto-selection.\n","        conf (float): Confidence threshold for detection model.\n","        iou (float): IoU threshold for filtering overlapping boxes in detection results.\n","        imgsz (int): Input image resize dimension.\n","        max_det (int): Maximum number of detections per image.\n","        classes (list[int], optional): Filter predictions to specified class IDs, returning only relevant detections.\n","        output_dir (str | Path, optional): Directory to save the annotated results. If None, creates a default\n","            directory based on the input data path.\n","\n","    Examples:\n","        >>> from ultralytics.data.annotator import auto_annotate\n","        >>> auto_annotate(data=\"ultralytics/assets\", det_model=\"yolo11n.pt\", sam_model=\"mobile_sam.pt\")\n","    \"\"\"\n","    det_model = YOLO(det_model)\n","    sam_model = SAM(sam_model)\n","\n","    data = Path(data)\n","    if not output_dir:\n","        output_dir = data.parent / f\"{data.stem}_auto_annotate_labels\"\n","    Path(output_dir).mkdir(exist_ok=True, parents=True)\n","\n","    det_results = det_model(\n","        data, stream=True, device=device, conf=conf, iou=iou, imgsz=imgsz, max_det=max_det, classes=classes\n","    )\n","\n","    for result in det_results:\n","        if class_ids := result.boxes.cls.int().tolist():  # Extract class IDs from detection results\n","            boxes = result.boxes.xyxy  # Boxes object for bbox outputs\n","            sam_results = sam_model(result.orig_img, bboxes=boxes, verbose=False, save=False, device=device)\n","            segments = sam_results[0].masks.xyn\n","\n","            with open(f\"{Path(output_dir) / Path(result.path).stem}.txt\", \"w\", encoding=\"utf-8\") as f:\n","                for i, s in enumerate(segments):\n","                    if s.any():\n","                        segment = map(str, s.reshape(-1).tolist())\n","                        f.write(f\"{class_ids[i]} \" + \" \".join(segment) + \"\\n\")"],"metadata":{"id":"5UBre8bXbWM4"},"execution_count":null,"outputs":[]}]}