{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN/8z0k7gxbTRP8GKDIKKPO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install sklearn\n","!pip install torch\n","!pip install pandas\n","# !pip install PIL"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"W-ayWcjp91ig","executionInfo":{"status":"ok","timestamp":1758875569569,"user_tz":-420,"elapsed":11937,"user":{"displayName":"Raihan Farid","userId":"10819737156521457225"}},"outputId":"7ec81a31-e353-4bdb-a9cc-442ae29abca8"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sklearn\n","  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"]}]},{"cell_type":"code","source":["!pip install tensorflow\n","!pip install proglearn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"oz_RqyCOAJMu","executionInfo":{"status":"ok","timestamp":1758875579420,"user_tz":-420,"elapsed":9848,"user":{"displayName":"Raihan Farid","userId":"10819737156521457225"}},"outputId":"1cbed421-80c7-4f1b-8b91-6a76b831c55b"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.0)\n","Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n","Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n","Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","Requirement already satisfied: proglearn in /usr/local/lib/python3.12/dist-packages (0.0.7)\n","Requirement already satisfied: tensorflow>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from proglearn) (2.19.0)\n","Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from proglearn) (1.6.1)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from proglearn) (1.16.2)\n","Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.12/dist-packages (from proglearn) (1.5.2)\n","Requirement already satisfied: numpy>=1.19.2 in /usr/local/lib/python3.12/dist-packages (from proglearn) (2.0.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.0->proglearn) (3.6.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.19.0->proglearn) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.19.0->proglearn) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.19.0->proglearn) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.19.0->proglearn) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.19.0->proglearn) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.19.0->proglearn) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.19.0->proglearn) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.19.0->proglearn) (25.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.19.0->proglearn) (5.29.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.19.0->proglearn) (2.32.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.19.0->proglearn) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.19.0->proglearn) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.19.0->proglearn) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.19.0->proglearn) (4.15.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.19.0->proglearn) (1.17.3)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.19.0->proglearn) (1.75.0)\n","Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.19.0->proglearn) (2.19.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.19.0->proglearn) (3.10.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.19.0->proglearn) (3.14.0)\n","Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=1.19.0->proglearn) (0.5.3)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow>=1.19.0->proglearn) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=1.19.0->proglearn) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=1.19.0->proglearn) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=1.19.0->proglearn) (0.17.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=1.19.0->proglearn) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=1.19.0->proglearn) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=1.19.0->proglearn) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=1.19.0->proglearn) (2025.8.3)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=1.19.0->proglearn) (3.9)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=1.19.0->proglearn) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=1.19.0->proglearn) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow>=1.19.0->proglearn) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=1.19.0->proglearn) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=1.19.0->proglearn) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=1.19.0->proglearn) (0.1.2)\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/milesial/Pytorch-UNet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ntU9CCkBRap","executionInfo":{"status":"ok","timestamp":1758875579553,"user_tz":-420,"elapsed":125,"user":{"displayName":"Raihan Farid","userId":"10819737156521457225"}},"outputId":"4a581a53-6371-45e8-95ac-92a5c5d2f314"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'Pytorch-UNet' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UGdHElAgDrmg","executionInfo":{"status":"ok","timestamp":1758875581195,"user_tz":-420,"elapsed":1640,"user":{"displayName":"Raihan Farid","userId":"10819737156521457225"}},"outputId":"9e885418-5d81-417e-8021-b6abf9f6d292"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import os\n","import shutil\n","from sklearn.model_selection import train_test_split\n","\n","# Define paths\n","dir_path = \"/content/drive/MyDrive/ObjectDetection-Tasks\"\n","filepath = os.path.join(dir_path, \"Task-for-model-2\")\n","dataset_path = os.path.join(filepath, 'retrainDataset/combinedRetrainData')  # New structured dataset location\n","\n","labels_source = os.path.join(dataset_path, \"labelData\") # Change this directory target into the converted directory\n","images_source = os.path.join(dataset_path,  \"imageData\") # add this if in another directory , \"captured_frames\""],"metadata":{"id":"d4aXSOz09k4f","executionInfo":{"status":"ok","timestamp":1758875581198,"user_tz":-420,"elapsed":1,"user":{"displayName":"Raihan Farid","userId":"10819737156521457225"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["### Load data"],"metadata":{"id":"ZsEPzQY3AEYP"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","# Define your paths\n","image_dir = images_source\n","label_dir = labels_source\n","\n","# Get all image files (assuming common image extensions)\n","image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n","image_files.sort()\n","\n","# Get corresponding label files (adjust extension as needed)\n","label_files = [f.replace('.jpg', '.txt').replace('.png', '.txt') for f in image_files]\n","\n","# Create full paths\n","image_paths = [os.path.join(image_dir, f) for f in image_files]\n","label_paths = [os.path.join(label_dir, f) for f in label_files]\n","\n","# First split: separate train + val from test (80% train+val, 20% test)\n","train_val_images, test_images, train_val_labels, test_labels = train_test_split(\n","    image_paths, label_paths, test_size=0.2, random_state=42, shuffle=True\n",")\n","\n","# Second split: separate train from val (75% train, 25% val of the 80%)\n","train_images, val_images, train_labels, val_labels = train_test_split(\n","    train_val_images, train_val_labels, test_size=0.25, random_state=42, shuffle=True\n",")\n","\n","print(f\"Train image samples: {len(train_images)} ; Train label samples: {len(train_labels)}\")\n","print(f\"Validation image samples: {len(val_images)} ; Validation label samples: {len(val_labels)}\")\n","print(f\"Test image samples: {len(test_images)} ; Test label samples: {len(test_labels)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XMQugWDp5xL_","executionInfo":{"status":"ok","timestamp":1758875581210,"user_tz":-420,"elapsed":10,"user":{"displayName":"Raihan Farid","userId":"10819737156521457225"}},"outputId":"39bd9560-020f-457b-e448-0021def22dd8"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Train image samples: 195 ; Train label samples: 195\n","Validation image samples: 65 ; Validation label samples: 65\n","Test image samples: 65 ; Test label samples: 65\n"]}]},{"cell_type":"code","source":["import json\n","\n","# After creating splits, save them to files\n","splits = {\n","    'train': {\n","        'images': train_images,\n","        'labels': train_labels\n","    },\n","    'val': {\n","        'images': val_images,\n","        'labels': val_labels\n","    },\n","    'test': {\n","        'images': test_images,\n","        'labels': test_labels\n","    }\n","}\n","\n","# Save to JSON\n","with open('/content/drive/MyDrive/ObjectDetection-Tasks/Task-for-model-2/retrainDataset/combinedRetrainData/dataset_splits.json', 'w') as f:\n","    # Convert Path objects to strings if necessary\n","    json_splits = {}\n","    for split_name, split_data in splits.items():\n","        json_splits[split_name] = {\n","            'images': [str(path) for path in split_data['images']],\n","            'labels': [str(path) for path in split_data['labels']]\n","        }\n","    json.dump(json_splits, f, indent=2)"],"metadata":{"id":"p-P3DjUJ-PV3","executionInfo":{"status":"ok","timestamp":1758875581213,"user_tz":-420,"elapsed":4,"user":{"displayName":"Raihan Farid","userId":"10819737156521457225"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["import json\n","# Load splits later\n","def load_splits(split_file):\n","    with open(split_file, 'r') as f:\n","        splits = json.load(f)\n","    return splits\n","\n","# Usage\n","splits = load_splits('/content/drive/MyDrive/ObjectDetection-Tasks/Task-for-model-2/retrainDataset/combinedRetrainData/dataset_splits.json')\n","train_images = splits['train']['images']\n","train_labels = splits['train']['labels']\n","test_images = splits['test']['images']\n","test_labels = splits['test']['labels']\n","val_images = splits['val']['images']\n","val_labels = splits['val']['labels']"],"metadata":{"id":"3mcbXCFm_w99","executionInfo":{"status":"ok","timestamp":1758875581217,"user_tz":-420,"elapsed":2,"user":{"displayName":"Raihan Farid","userId":"10819737156521457225"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["### Train model"],"metadata":{"id":"qpZbkykfAGDl"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers, Model, regularizers\n","\n","def build_regularized_unet(input_shape=(256, 256, 3), num_classes=5):\n","    \"\"\"\n","    Builds a simple U-Net inspired model with regularization for image segmentation.\n","    \"\"\"\n","    # Input layer\n","    inputs = tf.keras.Input(shape=input_shape)\n","\n","    # --- Data Augmentation (Applied as the first step) ---\n","    # Using a seed ensures the same transformation is applied to both image and mask :cite[1]\n","    x = layers.RandomFlip(mode=\"horizontal\", seed=42)(inputs)\n","    x = layers.RandomRotation(factor=0.1, seed=42)(x)\n","    x = layers.RandomContrast(factor=0.1, seed=42)(x)\n","\n","    # --- Encoder (Contracting Path) ---\n","    # Block 1\n","    c1 = layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(1e-4), padding='same')(x)\n","    c1 = layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(1e-4), padding='same')(c1)\n","    p1 = layers.MaxPooling2D((2, 2))(c1)\n","    p1 = layers.Dropout(0.2)(p1)  # Add dropout after pooling :cite[4]\n","\n","    # Block 2\n","    c2 = layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(1e-4), padding='same')(p1)\n","    c2 = layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(1e-4), padding='same')(c2)\n","    p2 = layers.MaxPooling2D((2, 2))(c2)\n","    p2 = layers.Dropout(0.2)(p2)\n","\n","    # --- Bottleneck ---\n","    bottleneck = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p2)\n","    bottleneck = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(bottleneck)\n","\n","    # --- Decoder (Expansive Path) ---\n","    # Upsample and concatenate with skip connection from c2\n","    u1 = layers.UpSampling2D((2, 2))(bottleneck)\n","    u1 = layers.concatenate([u1, c2])\n","    u1 = layers.Dropout(0.2)(u1)\n","    u1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u1)\n","    u1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u1)\n","\n","    # Final upsample and concatenate with skip connection from c1\n","    u2 = layers.UpSampling2D((2, 2))(u1)\n","    u2 = layers.concatenate([u2, c1])\n","    u2 = layers.Dropout(0.2)(u2)\n","    u2 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(u2)\n","    u2 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(u2)\n","\n","    # Output layer: Use a 1x1 convolution to map to the number of classes.\n","    # For binary segmentation, use 'sigmoid' activation; for multi-class, use 'softmax'.\n","    outputs = layers.Conv2D(num_classes, (1, 1), activation='softmax')(u2)\n","\n","    model = Model(inputs, outputs)\n","    return model\n","\n","# Build the model\n","model_unet = build_regularized_unet()\n","model_unet.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","model_unet.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"VBbxC-MeGigu","executionInfo":{"status":"ok","timestamp":1758875581418,"user_tz":-420,"elapsed":199,"user":{"displayName":"Raihan Farid","userId":"10819737156521457225"}},"outputId":"2cb764a1-8bcf-41b1-fc97-c8e62db4bf65"},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional_1\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ random_flip_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n","│ (\u001b[38;5;33mRandomFlip\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ random_rotation_1   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ random_flip_1[\u001b[38;5;34m0\u001b[0m]… │\n","│ (\u001b[38;5;33mRandomRotation\u001b[0m)    │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ random_contrast_1   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ random_rotation_… │\n","│ (\u001b[38;5;33mRandomContrast\u001b[0m)    │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │        \u001b[38;5;34m896\u001b[0m │ random_contrast_… │\n","│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │      \u001b[38;5;34m9,248\u001b[0m │ conv2d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ max_pooling2d_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_2[\u001b[38;5;34m…\u001b[0m │\n","│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │     \u001b[38;5;34m18,496\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │     \u001b[38;5;34m36,928\u001b[0m │ conv2d_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ max_pooling2d_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_3[\u001b[38;5;34m…\u001b[0m │\n","│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_15 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │     \u001b[38;5;34m73,856\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_16 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │    \u001b[38;5;34m147,584\u001b[0m │ conv2d_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ up_sampling2d_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","│ (\u001b[38;5;33mUpSampling2D\u001b[0m)      │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ concatenate_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ up_sampling2d_2[\u001b[38;5;34m…\u001b[0m │\n","│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m192\u001b[0m)              │            │ conv2d_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ concatenate_2[\u001b[38;5;34m0\u001b[0m]… │\n","│                     │ \u001b[38;5;34m192\u001b[0m)              │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │    \u001b[38;5;34m110,656\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │     \u001b[38;5;34m36,928\u001b[0m │ conv2d_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ up_sampling2d_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","│ (\u001b[38;5;33mUpSampling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ concatenate_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ up_sampling2d_3[\u001b[38;5;34m…\u001b[0m │\n","│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m96\u001b[0m)               │            │ conv2d_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ concatenate_3[\u001b[38;5;34m0\u001b[0m]… │\n","│                     │ \u001b[38;5;34m96\u001b[0m)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │     \u001b[38;5;34m27,680\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_20 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │      \u001b[38;5;34m9,248\u001b[0m │ conv2d_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_21 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m,  │        \u001b[38;5;34m165\u001b[0m │ conv2d_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","│                     │ \u001b[38;5;34m5\u001b[0m)                │            │                   │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ random_flip_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomFlip</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ random_rotation_1   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ random_flip_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomRotation</span>)    │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ random_contrast_1   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ random_rotation_… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomContrast</span>)    │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ random_contrast_… │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ conv2d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ max_pooling2d_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ conv2d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ max_pooling2d_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ conv2d_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ up_sampling2d_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ concatenate_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ up_sampling2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)              │            │ conv2d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)              │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">110,656</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ conv2d_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ up_sampling2d_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ concatenate_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ up_sampling2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)               │            │ conv2d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">27,680</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ conv2d_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ conv2d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span> │ conv2d_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                │            │                   │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m471,685\u001b[0m (1.80 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">471,685</span> (1.80 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m471,685\u001b[0m (1.80 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">471,685</span> (1.80 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["# Data Convert\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","import os\n","import numpy as np\n","\n","# Define your image size and batch size\n","img_size = (256, 256)\n","batch_size = 32\n","num_classes = 5 # Define the number of classes to match the model\n","\n","# 1. Prepare paths to your JPG images and segmentation masks\n","input_dir = \"/content/drive/MyDrive/ObjectDetection-Tasks/Task-for-model-2/retrainDataset/combinedRetrainData/imageData/\"\n","target_dir = \"/content/drive/MyDrive/ObjectDetection-Tasks/Task-for-model-2/retrainDataset/combinedRetrainData/labelData/\"\n","\n","# Get sorted list of file paths (assuming image and label have same base name)\n","image_files = sorted([fname for fname in os.listdir(input_dir) if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\"))])\n","label_files = sorted([fname for fname in os.listdir(target_dir) if fname.lower().endswith(\".txt\")])\n","\n","# Ensure there's a corresponding label file for each image file\n","input_img_paths = []\n","target_img_paths = []\n","\n","for img_file in image_files:\n","    base_name = os.path.splitext(img_file)[0]\n","    label_file = base_name + \".txt\"\n","    if label_file in label_files:\n","        input_img_paths.append(os.path.join(input_dir, img_file))\n","        target_img_paths.append(os.path.join(target_dir, label_file))\n","    else:\n","        print(f\"Warning: No corresponding label file found for image: {img_file}\")\n","\n","\n","print(f\"Number of samples found: {len(input_img_paths)}\")\n","\n","# 2. Create a TensorFlow Dataset\n","def get_dataset(batch_size, img_size, image_paths, label_paths):\n","    @tf.function # Use tf.function to compile the map function\n","    def load_img_masks(image_path, label_path):\n","        tf.print(f\"Processing image: {image_path}\") # Print current image path\n","\n","        # Read the raw image file\n","        img_raw = tf.io.read_file(image_path)\n","        tf.print(f\"Successfully read raw image file: {image_path}\")\n","\n","        # Try decoding as JPEG\n","        try:\n","            tf.print(f\"Attempting to decode as JPEG: {image_path}\")\n","            img = tf.io.decode_jpeg(img_raw, channels=3)\n","            tf.print(f\"Successfully decoded image as JPEG: {image_path}\")\n","\n","            img = tf.image.resize(img, img_size)\n","            img = tf.image.convert_image_dtype(img, \"float32\")\n","            tf.print(f\"Successfully resized and converted image: {image_path}\")\n","\n","\n","        except tf.errors.InvalidArgumentError as e:\n","            tf.print(f\"Error decoding image {image_path} as JPEG: {e}\")\n","            # If JPEG decoding fails, return dummy data\n","            return tf.zeros(list(img_size) + [3], dtype=tf.float32), tf.zeros(list(img_size) + [num_classes], dtype=tf.uint8)\n","        except Exception as e:\n","             tf.print(f\"An unexpected error occurred during JPEG image decoding for {image_path}: {e}\")\n","             # Return a dummy image and mask for other exceptions\n","             return tf.zeros(list(img_size) + [3], dtype=tf.float32), tf.zeros(list(img_size) + [num_classes], dtype=tf.uint8)\n","\n","\n","        # Read and process the YOLO label file into a mask\n","        # Assuming YOLO format: class_id center_x center_y width height (normalized)\n","        # Need to convert bounding boxes to a segmentation mask\n","\n","        # Create a blank mask image using TensorFlow with num_classes channels\n","        mask = tf.zeros(list(img_size) + [num_classes], dtype=tf.uint8)\n","\n","        # Read bounding boxes from the label file\n","        try:\n","            tf.print(f\"Processing label file: {label_path}\")\n","            # Read the content of the label file\n","            label_content = tf.io.read_file(label_path)\n","            label_lines = tf.strings.split(label_content, '\\n')\n","\n","            # Iterate over each line in the label file\n","            for line in tf.strings.split(label_lines, ' '):\n","                if tf.shape(line)[0] == 5: # Ensure the line has 5 elements (class_id, cx, cy, w, h)\n","                    class_id = tf.strings.to_number(line[0], out_type=tf.int32)\n","                    center_x = tf.strings.to_number(line[1], out_type=tf.float32)\n","                    center_y = tf.strings.to_number(line[2], out_type=tf.float32)\n","                    width = tf.strings.to_number(line[3], out_type=tf.float32)\n","                    height = tf.strings.to_number(line[4], out_type=tf.float32)\n","\n","                    # Convert normalized coordinates to pixel coordinates\n","                    img_width, img_height = tf.cast(img_size[1], tf.float32), tf.cast(img_size[0], tf.float32)\n","                    x_center = center_x * img_width\n","                    y_center = center_y * img_height\n","                    box_width = width * img_width\n","                    box_height = height * img_height\n","\n","                    x_min = tf.cast(x_center - box_width / 2, tf.int32)\n","                    y_min = tf.cast(y_center - box_height / 2, tf.int32)\n","                    x_max = tf.cast(x_center + box_width / 2, tf.int32)\n","                    y_max = tf.cast(y_center + box_height / 2, tf.int32)\n","\n","                    # Ensure coordinates are within image bounds\n","                    x_min = tf.maximum(0, x_min)\n","                    y_min = tf.maximum(0, y_min)\n","                    x_max = tf.minimum(tf.cast(img_width, tf.int32), x_max)\n","                    y_max = tf.minimum(tf.cast(img_height, tf.int32), y_max)\n","\n","\n","                    # Draw the bounding box on the mask (fill the corresponding class channel)\n","                    # Create a rectangle mask for the current bounding box for the specific class channel\n","                    if x_max > x_min and y_max > y_min and class_id < num_classes:\n","                        # Create a mask for this specific bounding box\n","                        box_mask = tf.ones([y_max - y_min, x_max - x_min, 1], dtype=tf.uint8) * 1\n","\n","                        # Pad this box mask back to the original image size\n","                        box_mask = tf.image.pad_to_bounding_box(\n","                            box_mask, y_min, x_min, img_size[0], img_size[1]\n","                        )\n","\n","                        # Expand dimensions to match mask shape (height, width, 1)\n","                        box_mask = tf.expand_dims(box_mask, axis=-1)\n","\n","                        # Scatter the box mask into the correct class channel of the main mask\n","                        # Create an update tensor with the box mask value for the specific class channel\n","                        updates = tf.ones([img_size[0], img_size[1]], dtype=tf.uint8) * 1 # Value to set in the channel\n","                        updates = updates * tf.squeeze(box_mask, axis=-1) # Apply the box mask shape\n","\n","                        # Create indices to scatter the updates into the correct channel\n","                        # Indices shape: (height, width, 1) - for each pixel, the class channel index\n","                        indices = tf.stack([\n","                            tf.range(img_size[0])[:, None] * tf.ones(img_size[1], dtype=tf.int32),\n","                            tf.ones([img_size[0], 1], dtype=tf.int32) * tf.range(img_size[1]),\n","                            tf.ones(img_size, dtype=tf.int32) * class_id # The class channel index\n","                        ], axis=-1)\n","\n","\n","                        # Use tf.tensor_scatter_nd_add or tf.tensor_scatter_nd_update\n","                        # tf.tensor_scatter_nd_add is good for overlapping boxes, adds 1 where they overlap\n","                        # tf.tensor_scatter_nd_update will just take the last value written to a pixel\n","                        # Let's use add for now to indicate overlaps, or you could use update if classes are mutually exclusive\n","                        # Note: scatter_nd_add requires input and updates to have the same type\n","                        updates = tf.cast(updates, tf.int32)\n","                        mask = tf.cast(mask, tf.int32)\n","                        mask = tf.tensor_scatter_nd_add(mask, indices, updates)\n","                        mask = tf.cast(mask, tf.uint8) # Convert back to uint8\n","\n","\n","                    else:\n","                        tf.print(f\"Warning: Invalid bounding box dimensions or class_id for {label_path}\")\n","\n","\n","        except Exception as e:\n","            tf.print(f\"Error processing label file {label_path}: {e}\")\n","            # Return an empty mask if there's an error\n","            mask = tf.zeros(list(img_size) + [num_classes], dtype=tf.uint8)\n","\n","\n","        # Resize to ensure correct size and dtype\n","        mask = tf.image.resize(mask, img_size, method=\"nearest\")\n","        mask = tf.image.convert_image_dtype(mask, \"uint8\")\n","\n","\n","        return img, mask\n","\n","    dataset = tf.data.Dataset.from_tensor_slices((input_img_paths, target_img_paths))\n","    dataset = dataset.map(load_img_masks, num_parallel_calls=tf.data.AUTOTUNE)\n","    dataset = dataset.batch(batch_size)\n","    return dataset\n","\n","# Determine the split points based on the number of samples found\n","total_samples = len(input_img_paths)\n","train_size = int(0.8 * total_samples)\n","val_size = int(0.1 * total_samples)\n","test_size = total_samples - train_size - val_size # Remaining for test\n","\n","# Create the training, validation, and test datasets\n","train_dataset = get_dataset(batch_size, img_size, input_img_paths[:train_size], target_img_paths[:train_size])\n","val_dataset = get_dataset(batch_size, img_size, input_img_paths[train_size:train_size + val_size], target_img_paths[train_size:train_size + val_size])\n","test_dataset = get_dataset(batch_size, img_size, input_img_paths[train_size + val_size:], target_img_paths[train_size + val_size:])\n","\n","# Print dataset sizes (adjusting for potential partial batches)\n","def print_dataset_size(dataset, name):\n","    cardinality = tf.data.experimental.cardinality(dataset)\n","    if cardinality == tf.data.experimental.INFINITE_CARDINALITY:\n","        print(f\"{name} dataset size: Infinite\")\n","    elif cardinality == tf.data.experimental.UNKNOWN_CARDINALITY:\n","        print(f\"{name} dataset size: Unknown\")\n","    else:\n","        print(f\"{name} dataset size: {cardinality.numpy() * batch_size}\")\n","\n","print_dataset_size(train_dataset, \"Train\")\n","print_dataset_size(val_dataset, \"Validation\")\n","print_dataset_size(test_dataset, \"Test\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l1BgnPvbJEpH","executionInfo":{"status":"ok","timestamp":1758876068743,"user_tz":-420,"elapsed":839,"user":{"displayName":"Raihan Farid","userId":"10819737156521457225"}},"outputId":"5ca20148-b90d-44b9-ee7b-0f40e0a5106e"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: No corresponding label file found for image: Salinan frame_20250920_114245_260051.jpg\n","Warning: No corresponding label file found for image: Salinan frame_20250923_161142_649935.jpg\n","Warning: No corresponding label file found for image: Salinan frame_20250923_161143_173291.jpg\n","Warning: No corresponding label file found for image: Salinan frame_20250923_161143_682450.jpg\n","Number of samples found: 321\n","Train dataset size: 352\n","Validation dataset size: 352\n","Test dataset size: 352\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"599fd8e1","executionInfo":{"status":"ok","timestamp":1758875984150,"user_tz":-420,"elapsed":625,"user":{"displayName":"Raihan Farid","userId":"10819737156521457225"}},"outputId":"178227a6-ce3b-4b2d-92c8-1ee3d3d225a8"},"source":["import tensorflow as tf\n","import os\n","\n","# Get the first image path from the list\n","if input_img_paths:\n","    sample_image_path = input_img_paths[0]\n","    print(f\"Attempting to load and decode sample image: {sample_image_path}\")\n","\n","    try:\n","        # Read the raw image file\n","        img_raw = tf.io.read_file(sample_image_path)\n","        print(f\"Successfully read raw image file: {sample_image_path}\")\n","\n","        # Attempt to decode the image\n","        try:\n","            # Try decoding as JPEG first, then PNG\n","            if tf.strings.regex_full_match(tf.strings.lower(sample_image_path), \".*\\.jpg$|.*\\.jpeg$\"):\n","                print(f\"Attempting to decode as JPEG: {sample_image_path}\")\n","                img = tf.io.decode_jpeg(img_raw, channels=3)\n","            elif tf.strings.regex_full_match(tf.strings.lower(sample_image_path), \".*\\.png$\"):\n","                print(f\"Attempting to decode as PNG: {sample_image_path}\")\n","                img = tf.io.decode_png(img_raw, channels=3)\n","            else:\n","                print(f\"Attempting to decode with decode_image: {sample_image_path}\")\n","                img = tf.image.decode_image(img_raw, channels=3)\n","\n","            print(f\"Successfully decoded sample image with shape: {img.shape}\")\n","\n","            # Optional: Display the decoded image to verify\n","            # import matplotlib.pyplot as plt\n","            # plt.imshow(img.numpy())\n","            # plt.title(\"Decoded Sample Image\")\n","            # plt.axis('off')\n","            # plt.show()\n","\n","        except tf.errors.InvalidArgumentError as e:\n","            print(f\"Error decoding sample image {sample_image_path}: {e}\")\n","        except Exception as e:\n","            print(f\"An unexpected error occurred during sample image decoding for {sample_image_path}: {e}\")\n","\n","    except Exception as e:\n","        print(f\"Error reading sample image file {sample_image_path}: {e}\")\n","\n","else:\n","    print(\"No image paths found in input_img_paths to test.\")"],"execution_count":50,"outputs":[{"output_type":"stream","name":"stderr","text":["<>:17: SyntaxWarning: invalid escape sequence '\\.'\n","<>:20: SyntaxWarning: invalid escape sequence '\\.'\n","<>:17: SyntaxWarning: invalid escape sequence '\\.'\n","<>:20: SyntaxWarning: invalid escape sequence '\\.'\n","/tmp/ipython-input-2941883299.py:17: SyntaxWarning: invalid escape sequence '\\.'\n","  if tf.strings.regex_full_match(tf.strings.lower(sample_image_path), \".*\\.jpg$|.*\\.jpeg$\"):\n","/tmp/ipython-input-2941883299.py:20: SyntaxWarning: invalid escape sequence '\\.'\n","  elif tf.strings.regex_full_match(tf.strings.lower(sample_image_path), \".*\\.png$\"):\n"]},{"output_type":"stream","name":"stdout","text":["Attempting to load and decode sample image: /content/drive/MyDrive/ObjectDetection-Tasks/Task-for-model-2/retrainDataset/combinedRetrainData/imageData/Salinan frame_20250920_114137_130922.jpg\n","Successfully read raw image file: /content/drive/MyDrive/ObjectDetection-Tasks/Task-for-model-2/retrainDataset/combinedRetrainData/imageData/Salinan frame_20250920_114137_130922.jpg\n","Attempting to decode as JPEG: /content/drive/MyDrive/ObjectDetection-Tasks/Task-for-model-2/retrainDataset/combinedRetrainData/imageData/Salinan frame_20250920_114137_130922.jpg\n","Successfully decoded sample image with shape: (1080, 1920, 3)\n"]}]},{"cell_type":"code","source":["# Train the model\n","history = model_unet.fit(\n","    train_dataset, # Use the TensorFlow dataset for training\n","    epochs=100,\n","    validation_data=val_dataset, # Use the TensorFlow dataset for validation\n","    callbacks=[\n","        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n","        tf.keras.callbacks.ReduceLROnPlateau(patience=5)  # Reduces learning rate when validation loss plateaus\n","    ]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"id":"JSubzvJIIIx2","executionInfo":{"status":"error","timestamp":1758876108792,"user_tz":-420,"elapsed":55,"user":{"displayName":"Raihan Farid","userId":"10819737156521457225"}},"outputId":"babf47ae-d3b2-4a6c-b787-26b7adce1225"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n"]},{"output_type":"error","ename":"ValueError","evalue":"Creating variables on a non-first call to a function decorated with tf.function.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4077932750.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model_unet.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Use the TensorFlow dataset for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Use the TensorFlow dataset for validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mmulti_step_on_iterator\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_execution\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 return tf.experimental.Optional.from_value(\n\u001b[0;32m--> 133\u001b[0;31m                     \u001b[0mone_step_on_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                 )\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Creating variables on a non-first call to a function decorated with tf.function."]}]},{"cell_type":"code","source":["# # Concept based on ProgLearn tutorial :cite[9]\n","# # Note: This is a structural outline; exact class names and APIs may need verification from the latest ProgLearn documentation.\n","\n","# # 1. Import necessary libraries\n","# import proglearn as PL\n","# from tensorflow.keras.models import Model\n","# from tensorflow.keras.layers import Input\n","# #from Unet import unet  # Custom function from the tutorial\n","# #from functions.scene_segmentation_nn_tutorial_functions import unet  # Custom function from the tutorial\n","# from sklearn.model_selection import train_test_split\n","\n","# # 2. Load and preprocess your data (X: images, y: labels)\n","# # ... (data loading code as shown in the tutorial)\n","# #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n","\n","# # 3. Define your base neural network model (e.g., U-Net)\n","# #input_layer = Input(shape=train_images[0].shape)\n","# base_model = model_unet(n_classes=5)  # n_classes from the tutorial example\n","\n","# # 4. Create the ProgLearn lifelong learner\n","# # The LifelongClassificationNetwork is intended to wrap a neural network.\n","# lifelong_nn = PL.LifelongClassificationNetwork(\n","#     base_model=base_model,  # Your compiled Keras model\n","#     # ... other parameters specific to the lifelong learner\n","# )\n","\n","# # 5. Train the pipeline using the lifelong learning interface\n","# # The .fit method should handle the sequential learning.\n","# lifelong_nn.fit(train_images, train_labels)\n","\n","# # 6. Make predictions and evaluate\n","# predictions = lifelong_nn.predict(test_images)\n","# # ... evaluation code"],"metadata":{"id":"Zzd6v-_8_-4H","executionInfo":{"status":"aborted","timestamp":1758875581543,"user_tz":-420,"elapsed":24261,"user":{"displayName":"Raihan Farid","userId":"10819737156521457225"}}},"execution_count":null,"outputs":[]}]}